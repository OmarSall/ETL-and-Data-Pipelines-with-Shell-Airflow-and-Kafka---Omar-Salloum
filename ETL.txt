echo "database" | cut -c1-4

echo "database" | cut -c1,5

cut -d ":" -f1 /etc/passwd 

cut -d":" -f1,3,6 /etc/passwd 

cut -d":" -f3-6 /etc/passwd 

echo "Shell Scripting" | tr "[a-z]" "[A-Z]" 

echo "Shell Scripting" | tr "[:lower:]" "[:upper:]" 

echo "Shell Scripting" | tr  "[A-Z]" "[a-z]" 

The -s option replaces a sequence of a repeated characters with a single occurrence of that character.

The command below replaces repeat occurrences of ‘space’ in the output of ps command with one ‘space’.

ps | tr -s " "

echo "My login pin is 5634" | tr -d "[:digit:]"

start_postgres

psql --username='username' --host='usually it is localhost'

->connect to db

\c template1

create table users(username varchar(50),userid int,homedirectory varchar(100));

\q

create bash file csv2.db.sh

# This script
# Extracts data from /etc/passwd file into a CSV file.

# The csv data file contains the user name, user id and 
# home directory of each user account defined in /etc/passwd

# Transforms the text delimiter from ":" to ",".
# Loads the data from the CSV file into a table in PostgreSQL database.

#Extract phase

echo "Extracting data"

# Extract the columns 1 (user name), 2 (user id) and 6 (home directory path)
#from /etc/passwd

cut -d":" -f1,3,6 /etc/passwd > extracted-data.txt
----------------------------------
bash csv2db.sh
cat extracted-data.txt

Whole file:
##############################################################################################################################
# This script
# Extracts data from /etc/passwd file into a CSV file.

# The csv data file contains the user name, user id and 
# home directory of each user account defined in /etc/passwd

# Transforms the text delimiter from ":" to ",".
# Loads the data from the CSV file into a table in PostgreSQL database.

#Extract phase

echo "Extracting data"

# Extract the columns 1 (user name), 2 (user id) and 6 (home directory path)
#from /etc/passwd

cut -d ":" -f1,3,6 /etc/passwd > extracted-data.txt

#Transform phase
echo "Transforming data"

#read the extracted data and replace the colons with comas

tr ":" "," < extracted-data.txt > transformed-data.csv

#Load phase

echo "Loading data"

# Send the instructions to connect to 'template1' and
# copy the file to the table 'users' through command pipeline.

------------------------------------------------------
COPY table_name FROM 'filename' DELIMITERS 'delimiter_character' FORMAT;
---------------------------------------------------------------------------
echo "\c template1;\COPY users  FROM '/home/project/transformed-data.csv' DELIMITERS ',' CSV;" | psql --username='username' --host='usually it is localhost'
##########################################################################################################################################

bash csv2db.sh

echo '\c template1; \\SELECT * from users;' | psql --username='username' --host='usually it is localhost'


------------------------------------PRACTICE EXERCISE-------------------------------------------------------

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Bash%20Scripting/ETL%20using%20shell%20scripting/web-server-access-log.txt.gz

start_postgres

psql -username='username' --host='usually it is localhost'

\c template1;

CREATE TABLE access_log(timestamp TIMESTAMP, latitude float, longitude float, visitor_id char(37));

\q


creating a .sh file
#############################################
# cp-access-log.sh
# This script downloads the file 'web-server-access-log.txt.gz'
# from "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Bash%20Scripting/ETL%20using%20shell%20scripting/".

# The script then extracts the .txt file using gunzip.

# The .txt file contains the timestamp, latitude, longitude 
# and visitor id apart from other data.

# Transforms the text delimeter from "#" to "," and saves to a csv file.
# Loads the data from the CSV file into the table 'access_log' in PostgreSQL database.

# Download the access log file

wget "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Bash%20Scripting/ETL%20using%20shell%20scripting/web-server-access-log.txt.gz"

# Unzip the file to extract the .txt file.
gunzip -f web-server-access-log.txt.gz

# Extract the columns 1 (timestamp), 2 (latitude), 3 (longitude) and 
# 4 (visitorid)

echo "Extracting data"

cut -d "#" -f1-4 web-server-access-log.txt > extracted-data.txt

#transform phase

echo "Transforming data"

tr "#" "," < extracted-data.txt > transformed-data.csv

# Load phase
echo "Loading data"

# Send the instructions to connect to 'template1' and
# copy the file to the table 'access_log' through command pipeline.

echo "\c template1;\COPY access_log  FROM '/home/project/transformed-data.csv' DELIMITERS ',' CSV HEADER;" | psql -username='username' --host='usually it is localhost'
##############################################################

bash cp-access-log.sh

echo '\c template1; \\SELECT * from access_log;' | psql --username='username' --host='usually it is localhost'


----------------------------LAB-----------------------------------------
start_airflow



