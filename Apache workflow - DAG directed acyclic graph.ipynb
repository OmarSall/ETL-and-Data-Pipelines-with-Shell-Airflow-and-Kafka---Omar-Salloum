{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda4b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147122ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "#DAG object, that we need to instantiate a DAG\n",
    "from airflow import DAG\n",
    "#Operators; we need this to write tasks\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "#This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining DAG arguments\n",
    "\n",
    "#You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee34ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAG definition\n",
    "dag = DAG(\n",
    "    dag_id='sample-etl-dag',\n",
    "    default_args=default_args,\n",
    "    description='Sample ETL DAG using Bash',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80813482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task definitions\n",
    "#first task - extract\n",
    "\n",
    "extract = BashOperator(\n",
    "    task_id='extract',\n",
    "    bash_command='echo \"extract\"',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "#second task - transform\n",
    "\n",
    "extract = BashOperator(\n",
    "    task_id='transform',\n",
    "    bash_command='echo\"transform\"',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "#third task - load\n",
    "\n",
    "extract = BashOperator(\n",
    "    task_id='load',\n",
    "    bash_command='echo\"load\"',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task pipeline\n",
    "extract >> transform >> load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This DAG has two tasks extract that extracts fields from /etc/passwd file and transform_and_load \n",
    "#that transforms and loads data into a file.\n",
    "\n",
    "# import the libraries\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "#defining DAG arguments\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "# defining the DAG\n",
    "# define the DAG\n",
    "dag = DAG(\n",
    "    'my-first-dag',\n",
    "    default_args=default_args,\n",
    "    description='My first DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "\n",
    "#define the first task\n",
    "\n",
    "extract = BashOperator(\n",
    "    task_id='extract',\n",
    "    bash_command='cut -d \":\" -f1,3,6 /etc/passwd > /home/project/airflow/dags/extracted-data.txt',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "#define the second task\n",
    "\n",
    "transform_and_load = BashOperator(\n",
    "    task_id='transform',\n",
    "    bash_command='tr \":\" \",\" < /home/project/airflow/dags/extracted-data.txt > /home/project/airflow/dags/transformed-data.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "#task pipeline\n",
    "extract >> transform_and_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845eca1a",
   "metadata": {},
   "source": [
    "/home/project/airflow/dags -> find a config file to submit a DAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbffc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo cp my_first_dag.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow dags list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: While submitting the dag that was created in the previous exercise, use sudo in the terminal before the command used to submit the dag.\n",
    "irflow dags list|grep \"my-first-dag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow tasks list my-first-dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c26a56",
   "metadata": {},
   "source": [
    "# Practice exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fc1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "\n",
    "\n",
    "#You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "# define the DAG\n",
    "dag = DAG(\n",
    "    'ETL_Server_Access_Log_Processing',\n",
    "    default_args=default_args,\n",
    "    description='My first DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "#define the first task - download task\n",
    "\n",
    "download = BashOperator(\n",
    "    task_id='download',\n",
    "    bash_command='wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "#define the second task - extract task\n",
    "\n",
    "extract = BashOperator(\n",
    "    task_id='extract',\n",
    "    bash_command='cut -d \"#\" -f1,4 /etc/passwd/web-server-access-log.txt > /home/project/airflow/dags/extracted.txt',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "#define the second task - transform\n",
    "\n",
    "transform = BashOperator(\n",
    "    task_id='transform',\n",
    "    bash_command='tr \"[a-z]\" \"[A-Z]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "#define the third task - load !!!!! The load task must compress the extracted and transformed data\n",
    "\n",
    "transform = BashOperator(\n",
    "    task_id='load',\n",
    "    bash_command='zip log.zip capitalized.txt',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task pipeline\n",
    "\n",
    "download >> extract >> transform >> load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the DAG.\n",
    "\n",
    " sudo cp ETL_Server_Access_Log_Processing.py $AIRFLOW_HOME/dags\n",
    "\n",
    "    \n",
    "sudo cp ETL_toll_data.py $AIRFLOW_HOME/dags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify if the DAG is submitted\n",
    "\n",
    "airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9be53",
   "metadata": {},
   "source": [
    "# ETL process  with DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "create -> my_first_dag.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"extract_transform_load\"\n",
    "cut -d \":\" -f1,3,6 /etc/passwd > /home/project/airflow/dags/extracted-data.txt\n",
    "\n",
    "tr \":\" \",\" < /home/project/airflow/dags/extracted-data.txt > /home/project/airflow/dags/transformed-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "#defining DAG arguments\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# defining the DAG\n",
    "\n",
    "# define the DAG\n",
    "dag = DAG(\n",
    "    'my-first-dag',\n",
    "    default_args=default_args,\n",
    "    description='My first DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# define the task **extract_transform_and_load** to call shell script\n",
    "\n",
    "#calling the shell script\n",
    "extract_transform_load = BashOperator(\n",
    "    task_id=\"extract_transform_load\",\n",
    "    bash_command=\"/home/project/airflow/dags/my_first_dag.sh \",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task pipeline\n",
    "extract_transform_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo  cp my_first_dag.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd airflow/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa515d",
   "metadata": {},
   "outputs": [],
   "source": [
    " chmod 777 my_first_dag.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "airflow dags list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow dags list|grep \"my-first-dag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74904681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "airflow tasks list my-first-dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59004733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3112a9d2",
   "metadata": {},
   "source": [
    "# Practice exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0436dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f614b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL_Server_Access_Log_Processing.sh\n",
    "\n",
    "#!/bin/bash\n",
    "echo \"extract_transform_load\"\n",
    "# cut command to extract the fields timestamp and visitorid writes to a new file extracted.txt\n",
    "cut -f1,4 -d\"#\" /home/project/airflow/dags/web-server-access-log.txt > /home/project/airflow/dags/extracted.txt\n",
    "\n",
    "# tr command to transform by capitalizing the visitorid.\n",
    "tr \"[a-z]\" \"[A-Z]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.txt\n",
    "\n",
    "# tar command to compress the extracted and transformed data.\n",
    "tar -czvf /home/project/airflow/dags/log.tar.gz /home/project/airflow/dags/capitalized.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL_Server_Access_Log_Processing.py\n",
    "\n",
    "# import the libraries\n",
    "\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "\n",
    "#defining DAG arguments\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# defining the DAG\n",
    "\n",
    "# define the DAG\n",
    "dag = DAG(\n",
    "    'ETL_Server_Access_Log_Processing',\n",
    "    default_args=default_args,\n",
    "    description='My first DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "\n",
    "# define the tasks\n",
    "\n",
    "#define the task named extract_transform_and_load to call the shell script\n",
    "#calling the shell script\n",
    "extract_transform_and_load = BashOperator(\n",
    "    task_id=\"extract_transform_and_load\",\n",
    "    bash_command=\"/home/project/airflow/dags/ETL_Server_Access_Log_Processing.sh \",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "# task pipeline\n",
    "\n",
    "extract_transform_and_load\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    " sudo cp  ETL_Server_Access_Log_Processing.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da8117",
   "metadata": {},
   "outputs": [],
   "source": [
    " sudo cp  ETL_Server_Access_Log_Processing.sh $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932a251",
   "metadata": {},
   "outputs": [],
   "source": [
    " cd airflow/dags\n",
    " chmod 777 ETL_Server_Access_Log_Processing.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21acf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62b6e7",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fb8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_airflow\n",
    "#submit a dummy dag\n",
    "\n",
    "#create new file dummy_dag.py\n",
    " # import the libraries\n",
    "\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "#defining DAG arguments\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# defining the DAG\n",
    "dag = DAG(\n",
    "    'dummy_dag',\n",
    "    default_args=default_args,\n",
    "    description='My first DAG',\n",
    "    schedule_interval=timedelta(minutes=1),\n",
    ")\n",
    "\n",
    "# define the tasks\n",
    "\n",
    "# define the first task\n",
    "\n",
    "task1 = BashOperator(\n",
    "    task_id='task1',\n",
    "    bash_command='sleep 1',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# define the second task\n",
    "task2 = BashOperator(\n",
    "    task_id='task2',\n",
    "    bash_command='sleep 2',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# define the third task\n",
    "task3 = BashOperator(\n",
    "    task_id='task3',\n",
    "    bash_command='sleep 3',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task pipeline\n",
    "task1 >> task2 >> task3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61427bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit a dag file\n",
    "\n",
    "sudo cp dummy_dag.py airflow/dags\n",
    "\n",
    "airflow dags list\n",
    "\n",
    "airflow tasks list dummy_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b27753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7013f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834878e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
